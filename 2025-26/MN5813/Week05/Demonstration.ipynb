{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Advanced Pandas (Demonstration)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "_This notebook builds on the [Pandas](https://pandas.pydata.org/) fundamentals you learnt in Week 04, demonstrating advanced techniques for data reshaping and analysis. We'll focus on the concept of \"tidy\" data and powerful methods for transforming your datasets._\n",
    "\n",
    "Note: This Jupyter Notebook was originally compiled by Alex Reppel (AR) based on conversations with [ClaudeAI](https://claude.ai/) *(version 3.5 Sonnet)*. For this year's materials, further revisions were made using [Claude Code](https://www.anthropic.com/claude-code) *(Sonnet 4.5)*, including updated documentation and git commit messages."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set display options\n",
    "pd.set_option(\"display.max_columns\", 50)\n",
    "plt.style.use(\"ggplot\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## \ud83c\udfaf CORE CONTENT (Essential for Exercises)\n",
    "\n",
    "**Estimated time**: 50-60 minutes\n",
    "\n",
    "The sections below cover essential advanced Pandas techniques you'll need for the exercises:\n",
    "- Understanding tidy data principles\n",
    "- Reshaping data with `melt()` and `pivot()`\n",
    "- Creating pivot tables with `pivot_table()`\n",
    "- Basic data aggregation\n",
    "\n",
    "Work through these sections carefully. The exercises will require you to apply these reshaping techniques.\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## The concept of \"tidy\" data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Before we explore advanced data manipulation and analysis techniques, let's cover a fundamental concept: \"Tidy\" data _(vs. \"messy\" data)_. This concept, introduced by Hadley Wickham, has become a cornerstone of modern data analysis."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### What is tidy data?\n",
    "\n",
    "Tidy data is a standard way of mapping the meaning of a dataset to its structure. A dataset is messy or tidy depending on how rows, columns and tables are matched up with observations, variables and types. In tidy data:\n",
    "\n",
    "1. Each variable forms a column\n",
    "2. Each observation forms a row\n",
    "3. Each type of observational unit forms a table"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Principles of tidy data\n",
    "\n",
    "1. Column headers are variable names\n",
    "   - Not values\n",
    "   - Not complex descriptions\n",
    "   - Should be clear and concise\n",
    "2. Variables are organised in columns\n",
    "   - Each column contains one and only one variable\n",
    "   - All entries in a column should be of the same type\n",
    "   - No mixing of different units or types of information\n",
    "3. Observations are organised in rows\n",
    "   - Each row represents one and only one observation\n",
    "   - All values in a row should correspond to the same observational unit\n",
    "4. Each type of observational unit forms a table\n",
    "   - Different types of units should be stored in separate tables\n",
    "   - Tables can be linked through shared identifiers"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Common \"messy\" data problems\n",
    "\n",
    "Below are several examples comparing \"tidy\" vs. \"messy\" data to illustrate some of the issues related to \"messy\" data."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### \"Messy\" problem #1\n",
    "\n",
    "Column headers are values, not variable names."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Messy\n",
    "pd.DataFrame({\n",
    "    \"year\": [2020, 2021],\n",
    "    \"q1\": [100, 110],\n",
    "    \"q2\": [120, 130],\n",
    "    \"q3\": [140, 150],\n",
    "    \"q4\": [160, 170]\n",
    "})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Tidy\n",
    "pd.DataFrame({\n",
    "    \"year\": [2020, 2020, 2020, 2020, 2021, 2021, 2021, 2021],\n",
    "    \"quarter\": [\"Q1\", \"Q2\", \"Q3\", \"Q4\", \"Q1\", \"Q2\", \"Q3\", \"Q4\"],\n",
    "    \"value\": [100, 120, 140, 160, 110, 130, 150, 170]\n",
    "})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### \"Messy\" problem #2\n",
    "\n",
    "Multiple variables are stored in one column."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Messy\n",
    "pd.DataFrame({\n",
    "    \"id\": [1, 2],\n",
    "    \"age_gender\": [\"25_M\", \"30_F\"]\n",
    "})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Tidy\n",
    "pd.DataFrame({\n",
    "    \"id\": [1, 2],\n",
    "    \"age\": [25, 30],\n",
    "    \"gender\": [\"M\", \"F\"]\n",
    "})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### \"Messy\" problem #3\n",
    "\n",
    "Variables are stored in both rows and columns."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Messy data _(temperature readings)_:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "| location | measurement        | value |\n",
    "|----------|-------------------|-------|\n",
    "| LA       | temperature_max   | 85    |\n",
    "| LA       | temperature_min   | 65    |\n",
    "| NYC      | temperature_max   | 72    |\n",
    "| NYC      | temperature_min   | 55    |"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Tidy data _(temperature readings)_:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "| location | max_temp | min_temp |\n",
    "|----------|----------|----------|\n",
    "| LA       | 85       | 65       |\n",
    "| NYC      | 72       | 55       |"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### A (slightly) more detailed example"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's look at another example. While the \"messy\" version below is easily readable, it is considered \"messy\" because it violates the first principle of \"tidy\" data: \"Each variable forms a column.\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "| Year | Country | Men | Women | Non-binary | Other |\n",
    "|------|---------|-----|-------|------------|-------|\n",
    "| 2020 | USA     | 10  | 12    | 3          | 2     |\n",
    "| 2020 | Canada  | 8   | 9     | 2          | 1     |\n",
    "| 2021 | USA     | 11  | 13    | 4          | 3     |\n",
    "| 2021 | Canada  | 9   | 10    | 3          | 2     |"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's analyse what the variables really are in this dataset:\n",
    "\n",
    "1. `Year` _(currently a column - good!)_\n",
    "2. `Country` _(currently a column - good!)_\n",
    "3. `Gender` _(currently split across four columns - thus violating the first principle of \"tidy\" data!)_\n",
    "4. `Count` / `Value` _(the numbers in the cells of the \"messy\" version)_\n",
    "\n",
    "The issue is that `Gender` is not being treated as a single variable; instead, it's spread across four columns (`Men`, `Women`, `Non-binary`, `Other`). In \"tidy\" data, `Gender` should be a single variable in one column, with the possible values being `Men`, `Women`, `Non-binary`, and `Other`.\n",
    "\n",
    "Think about it this way:\n",
    "\n",
    "1. What if we needed to add another gender category? _We'd have to add a new column!_\n",
    "2. What if we wanted to calculate the percentage by gender? _We'd have to reference multiple columns!_\n",
    "3. What if we wanted to plot gender distribution? _We'd need to reshape the data first!_\n",
    "\n",
    "Now compare the above with this example of \"tidy\" data:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "| Year | Country | Gender     | Value |\n",
    "|------|---------|------------|-------|\n",
    "| 2020 | USA     | Men        | 10    |\n",
    "| 2020 | USA     | Women      | 12    |\n",
    "| 2020 | USA     | Non-binary | 3     |\n",
    "| 2020 | USA     | Other      | 2     |\n",
    "| 2020 | Canada  | Men        | 8     |\n",
    "| 2020 | Canada  | Women      | 9     |\n",
    "| 2020 | Canada  | Non-binary | 2     |\n",
    "| 2020 | Canada  | Other      | 1     |\n",
    "| 2021 | USA     | Men        | 11    |\n",
    "| 2021 | USA     | Women      | 13    |\n",
    "| 2021 | USA     | Non-binary | 4     |\n",
    "| 2021 | USA     | Other      | 3     |\n",
    "| 2021 | Canada  | Men        | 9     |\n",
    "| 2021 | Canada  | Women      | 10    |\n",
    "| 2021 | Canada  | Non-binary | 3     |\n",
    "| 2021 | Canada  | Other      | 2     |"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this format:\n",
    "\n",
    "1. Each variable _(`Year`, `Country`, `Gender`, `Value`)_ is a single column\n",
    "2. Each observation is a single row\n",
    "3. Each cell contains a single value\n",
    "4. Analysis is more straightforward _(e.g., `groupby(\"Gender\").sum()`)_\n",
    "5. It's easier to add new `gender` categories _(just add new rows)_\n",
    "5. It's more compatible with visualisation libraries\n",
    "\n",
    "This is similar to the way we'd store the data in a database; you wouldn't typically have separate columns for each possible value of a categorical variable. After a brief summary below, you will see how to convert between these formats using Pandas in the next block. This includes using `melt()` to go from the \"messy\" to \"tidy\" format, and `pivot()` to go back if needed."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Summary: Why is \"tidy\" data important?\n",
    "\n",
    "#### Consistency\n",
    "\n",
    "- Standardized way to structure data\n",
    "- Makes data easier to understand\n",
    "- Facilitates collaboration\n",
    "- Reduces errors in analysis\n",
    "\n",
    "#### Ease of manipulation\n",
    "\n",
    "- Most Pandas functions expect tidy data\n",
    "- Simplified data transformation\n",
    "- More intuitive filtering and grouping\n",
    "- Easier to reshape when needed\n",
    "\n",
    "#### Analysis ready\n",
    "\n",
    "- Compatible with most statistical models\n",
    "- Ready for machine learning algorithms\n",
    "- Easier to identify patterns and trends\n",
    "- Facilitates feature engineering\n",
    "\n",
    "#### Visualisation friendly\n",
    "\n",
    "- Works well with plotting libraries\n",
    "- Easier to create meaningful visualisations\n",
    "- Better control over aesthetic mappings"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Best practices for \"tidying\" data\n",
    "\n",
    "#### Document your tidying process\n",
    "\n",
    "- Keep track of transformations\n",
    "- Note any assumptions made\n",
    "- Document reasons for choices\n",
    "\n",
    "#### Preserve raw data\n",
    "\n",
    "- Keep original data unchanged\n",
    "- Create copies for tidying\n",
    "- Maintain data lineage\n",
    "\n",
    "#### Validate after tidying\n",
    "\n",
    "- Check for missing values\n",
    "- Verify data types\n",
    "- Confirm unique identifiers\n",
    "- Test relationships between variables\n",
    "\n",
    "#### Consider the end use\n",
    "\n",
    "- Think about analysis needs\n",
    "- Plan for visualisation requirements\n",
    "- Account for model requirements\n",
    "\n",
    "_(**Remember:** While tidy data is ideal for many analyses, sometimes other formats might be more appropriate for specific tasks. The key is to understand when and why to use each format.)_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Tidying data with Pandas\n",
    "\n",
    "Pandas provides several functions to help tidy your data _(which we will be introducing/using throughout this module)_:\n",
    "\n",
    "1. `melt()`: Unpivot a `DataFrame` from wide to long format.\n",
    "2. `pivot()`: Reshape data from long to wide format.\n",
    "3. `stack()` and `unstack()`: Reshape data by pivoting a level of the _(possibly hierarchical)_ column labels.\n",
    "\n",
    "Remember, the goal is to structure your data so that it's easy to work with for your specific analysis needs. Sometimes, the most convenient structure for your analysis might not be the tidiest format, so use these principles as guidelines rather than strict rules."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Creating a data frame\n",
    "\n",
    "With this out of the way, let's start by creating a sample `DataFrame` to work with throughout the demonstration."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# More complex sample data\n",
    "data = {\n",
    "    \"name\": [\"Alice\", \"Bob\", \"Carol\", \"Dan\", \"Eve\", \"Frank\", \"Grace\", \"Henry\", \n",
    "             \"Ivy\", \"Jack\", \"Kelly\", \"Liam\", \"Maya\", \"Noah\", \"Olivia\"],\n",
    "    \n",
    "    \"age\": [25, 30, 35, 40, 45, 28, 33, 38, 42, 47, 29, 34, 39, 44, 31],\n",
    "    \n",
    "    \"city\": [\"New York\", \"London\", \"Paris\", \"Tokyo\", \"Sydney\", \"Berlin\", \"Toronto\",\n",
    "             \"Madrid\", \"Rome\", \"Amsterdam\", \"Singapore\", \"Dubai\", \"Moscow\", \n",
    "             \"Stockholm\", \"Vancouver\"],\n",
    "    \n",
    "    \"department\": [\"Sales\", \"IT\", \"HR\", \"Sales\", \"IT\", \"HR\", \"Sales\", \"IT\", \"HR\",\n",
    "                  \"Sales\", \"IT\", \"HR\", \"Sales\", \"IT\", \"HR\"],\n",
    "    \n",
    "    \"salary_2022\": [50000, 60000, 70000, 80000, 90000, 55000, 65000, 75000,\n",
    "                    85000, 95000, 52000, 62000, 72000, 82000, 92000],\n",
    "    \n",
    "    \"salary_2023\": [52000, 63000, 73000, 84000, 94000, 57000, 68000, 78000,\n",
    "                    89000, 99000, 54000, 65000, 75000, 86000, 96000],\n",
    "    \n",
    "    \"performance_Q1\": [4.5, 3.8, 4.2, 3.9, 4.7, 4.1, 3.7, 4.3, 4.0, 4.6,\n",
    "                      3.9, 4.4, 4.2, 3.8, 4.5],\n",
    "    \n",
    "    \"performance_Q2\": [4.3, 4.0, 4.1, 4.2, 4.5, 3.9, 3.8, 4.4, 4.1, 4.7,\n",
    "                      4.0, 4.3, 4.1, 3.9, 4.4],\n",
    "    \n",
    "    \"projects_completed\": [5, 7, 4, 6, 8, 5, 6, 7, 4, 8, 5, 6, 7, 5, 6],\n",
    "    \n",
    "    \"training_hours\": [20, 35, 25, 30, 40, 22, 28, 35, 26, 38, 24, 32, 27, 36, 29]\n",
    "}\n",
    "\n",
    "# Create DataFrame\n",
    "df = pd.DataFrame(data)\n",
    "\n",
    "# Add some missing values (to make the DataFrame more realistic)\n",
    "df.loc[2, \"performance_Q2\"] = np.nan\n",
    "df.loc[5, \"training_hours\"] = np.nan\n",
    "df.loc[8, \"projects_completed\"] = np.nan\n",
    "df.loc[11, \"performance_Q1\"] = np.nan"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"First few rows of the original DataFrame:\")\n",
    "print(df.head(3))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Tools for tidying data in Pandas: `melt()`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can use the `melt()` function to convert \"wide\" data to \"long\" format _(as in the example above)_.\n",
    "\n",
    "#### Purpose of `melt()`\n",
    "\n",
    "- Transforms \"wide\" format data into \"long\" format\n",
    "- Converts columns into rows\n",
    "- Useful for time series analysis and visualization\n",
    "\n",
    "#### Key parameters used\n",
    "\n",
    "- `id_vars`: Columns to keep as identifier variables _(`name` in our case)_\n",
    "- `value_vars`: Columns to unpivot _(`salary_2022`, `salary_2023`)_\n",
    "- `var_name`: Name for the new column containing former column names _(`year`)_\n",
    "- `value_name`: Name for the new column containing the values _(`salary`)_\n",
    "\n",
    "#### The transformation\n",
    "\n",
    "- Before: Each row has one person with multiple columns for years\n",
    "- After: Each row has one person-year combination\n",
    "- Two rows per person _(one for each year)_\n",
    "\n",
    "#### Benefits\n",
    "\n",
    "- Easier to analyse trends over time\n",
    "- Better format for many visualisation libraries\n",
    "- More compatible with statistical analyses\n",
    "- Easier to add new years without adding columns"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Example"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Converting salary columns to long format\n",
    "df_melted_salary = pd.melt(\n",
    "    df,\n",
    "    id_vars=[\"name\"],\n",
    "    value_vars=[\"salary_2022\", \"salary_2023\"],\n",
    "    var_name=\"year\",\n",
    "    value_name=\"salary\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"\\nMelted salary data (first 3 rows):\")\n",
    "print(df_melted_salary.head(3))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Clean up the year column to remove \"salary_\" prefix\n",
    "df_melted_salary[\"year\"] = df_melted_salary[\"year\"].str.replace(\"salary_\", \"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"\\nCleaned melted data (first 3 rows):\")\n",
    "print(df_melted_salary.head(3))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Tools for tidying data in Pandas: `pivot()`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The `pivot()` function is essentially the inverse operation of `melt()`. While `melt()` transforms wide data into long format, `pivot()` reshapes long data back into wide format. \n",
    "\n",
    "#### Basic pivoting\n",
    "\n",
    "- Takes long-format data and converts it to wide format\n",
    "- Creates a new column for each unique value in the `columns` parameter\n",
    "- Values are reorganized based on the index and new columns\n",
    "\n",
    "#### Advanced features\n",
    "\n",
    "- Can handle multiple index levels (`index=[\"name\", \"department\"]`)\n",
    "- Can pivot multiple value columns simultaneously\n",
    "- Automatically handles duplicates through aggregation\n",
    "\n",
    "#### Common use cases\n",
    "\n",
    "- Converting time series data from long to wide format\n",
    "- Creating cross-tabulations\n",
    "- Preparing data for visualization\n",
    "- Building summary tables\n",
    "\n",
    "#### Best practices\n",
    "\n",
    "- Always check for duplicate combinations of index and columns\n",
    "- Consider using `pivot_table()` for more complex pivoting operations\n",
    "- Use `reset_index()` after pivoting if you need the index as a regular column"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Example"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A common workflow is to (a) melt data to long format for certain operations, and then (b) to pivot it back to wide format for other operations. This is illustrated with the next code block where we calculate the changes in salary between two years."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Demonstrate pivot() to get back to wide format\n",
    "df_pivoted = df_melted_salary.pivot(\n",
    "    index=\"name\",     # Rows - what will identify each record\n",
    "    columns=\"year\",   # Columns - what will become the new columns\n",
    "    values=\"salary\"   # Values - what will fill the cells\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here's what each parameter does:\n",
    "\n",
    "1. **index:** Specifies which column(s) will identify each row\n",
    "2. **columns:** Specifies which column contains the values that will become new column headers\n",
    "3. **values:** Specifies which column contains the values that will fill the table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Pivoted (wide) format:\")\n",
    "print(df_pivoted.head())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Summary: `melt()` and `pivot()`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This code example combining the `melt()` and `pivot()` functions demonstrates a common data analysis pattern:\n",
    "\n",
    "1. Start with wide format: Each `person` has one row with multiple `year` columns\n",
    "2. Melt to long format: Each `person`-`year` combination gets its own row\n",
    "3. Pivot back to wide format: Restructure back to one row per `person`\n",
    "4. Perform calculations: Calculate differences across `years`\n",
    "\n",
    "The key difference between the original wide format and the final wide format is that:\n",
    "\n",
    "1. We've cleaned up the data _(removed `salary_` prefix from `year` columns)_\n",
    "2. We can now easily add the calculation for `salary` increase\n",
    "\n",
    "Why do this?\n",
    "\n",
    "Sometimes you need:\n",
    "\n",
    "1. Long format: for plotting time series, statistical analysis, or certain types of aggregations\n",
    "2. Wide format: for calculating differences between columns or creating summary statistics\n",
    "\n",
    "This pattern of melting and then pivoting is particularly useful when you need to:\n",
    "\n",
    "1. Clean or transform your data _(easier in long format)_\n",
    "2. Then perform calculations across `years` / `categories` _(easier in wide format)_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Tools for tidying data in Pandas: `pivot_table()`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Above, we've used `pivot()` to convert data from \"long\" into \"wide\" format. There's also `pivot_table()` for more advanced operations. Differences between `pivot()` and `pivot_table()`:\n",
    "\n",
    "1. `pivot()`: Simpler function for basic reshaping; doesn't handle duplicate values\n",
    "2. `pivot_table()`: More powerful function that can:\n",
    "\n",
    "   - Handle duplicate values through aggregation\n",
    "   - Pivot multiple value columns\n",
    "   - Create multi-level indexes and columns\n",
    "   - Apply different aggregation functions\n",
    "\n",
    "Pivot tables are used to summarise and aggregate data inside `DataFrames`. They're especially useful for data with multiple dimensions. Pivot tables are a useful data analysis tools that to (re)structure and summarise data by:\n",
    "\n",
    "1. Reorganising data from a \"long\" format to a \"wide\" format _(as you've seen above)_\n",
    "2. Performing calculations across different dimensions of your data\n",
    "3. Creating cross-tabulations of your data _(\"crosstabs\")_\n",
    "\n",
    "In the example below, we are essentially reversing the coversation of \"wide\" _(which we said about is often considered \"messy\")_ data into \"long\" _(typically \"tidy\")_ data. Sometimes, that's exactly what we need."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Example\n",
    "\n",
    "The `pivot_table()` function is particularly useful for HR analytics and business reporting, which is what we'll be looking at next."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example data in long format:\n",
    "sales_data = pd.DataFrame({\n",
    "    \"date\": pd.date_range(start=\"2024-10-25\", periods=12),\n",
    "    \"product\": [\"A\", \"B\"] * 6,\n",
    "    \"sales\": np.random.randint(100, 1000, 12)\n",
    "})\n",
    "\n",
    "print(\"Data in long format:\")\n",
    "print(sales_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Pivot table in wide format:\n",
    "pivot_table = pd.pivot_table(\n",
    "    sales_data, \n",
    "    values=\"sales\",      # What we're measuring\n",
    "    index=\"date\",        # Rows\n",
    "    columns=\"product\",   # Columns\n",
    "    aggfunc=\"sum\")       # How to aggregate\n",
    "\n",
    "print(\"Pivot table in wide format:\")\n",
    "print(pivot_table)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Summary: `pivot_table()`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Key components\n",
    "\n",
    "1. `values`: The data you want to analyze _(like sales numbers)_\n",
    "2. `index`: The categories you want as rows _(like dates)_\n",
    "3. `columns`: The categories you want as columns _(like products)_\n",
    "4. `aggfunc`: How to combine the data _(`sum`, `mean`, `count`, etc.)_\n",
    "\n",
    "#### Practical use cases\n",
    "\n",
    "1. Sales analysis by product and time period\n",
    "2. Customer behavior analysis across different segments\n",
    "3. Performance metrics across departments and regions\n",
    "\n",
    "#### Additions\n",
    "\n",
    "1. Using multiple index/column levels\n",
    "2. Applying different aggregate functions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Example"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Basic pivot table: Average salary by department and year\n",
    "print(\"Average salary by department and year:\")\n",
    "basic_pivot = pd.pivot_table(\n",
    "    df_melted_salary.merge(df[[\"name\", \"department\"]], on=\"name\"),\n",
    "    values=\"salary\",\n",
    "    index=\"department\",\n",
    "    columns=\"year\",\n",
    "    aggfunc=\"mean\"\n",
    ").round(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(basic_pivot)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# More complex pivot table: Multiple metrics by department\n",
    "print(\"Department performance metrics:\")\n",
    "dept_pivot = pd.pivot_table(\n",
    "    df,\n",
    "    values=[\"salary_2022\", \"performance_Q1\", \"training_hours\", \"projects_completed\"],\n",
    "    index=\"department\",\n",
    "    aggfunc={\n",
    "        \"salary_2022\": \"mean\",\n",
    "        \"performance_Q1\": [\"mean\", \"min\", \"max\"],\n",
    "        \"training_hours\": \"sum\",\n",
    "        \"projects_completed\": \"sum\"\n",
    "    }\n",
    ").round(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(dept_pivot)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Advanced pivot table: Performance metrics by city and department\n",
    "print(\"City and department performance analysis:\")\n",
    "location_pivot = pd.pivot_table(\n",
    "    df,\n",
    "    values=[\"performance_Q1\", \"performance_Q2\"],\n",
    "    index=[\"city\"],\n",
    "    columns=[\"department\"],\n",
    "    aggfunc=\"mean\",\n",
    "    fill_value=0\n",
    ").round(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(location_pivot)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Pivot table with margins (subtotals)\n",
    "print(\"Salary analysis with subtotals:\")\n",
    "salary_pivot = pd.pivot_table(\n",
    "    df,\n",
    "    values=[\"salary_2022\", \"salary_2023\"],\n",
    "    index=\"department\",\n",
    "    aggfunc=\"mean\",\n",
    "    margins=True,\n",
    "    margins_name=\"Overall Average\"\n",
    ").round(2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## \ud83d\udcda SUPPLEMENTARY CONTENT (Advanced Techniques)\n",
    "\n",
    "**Estimated time**: 15-25 minutes\n",
    "\n",
    "The sections below cover more advanced Pandas techniques:\n",
    "- `stack()` and `unstack()` for hierarchical data\n",
    "- Window functions and rolling calculations\n",
    "- Working with MultiIndex\n",
    "- Time series resampling\n",
    "- Advanced aggregations\n",
    "\n",
    "These topics are **not required for the exercises** but provide deeper insights for complex analyses. Focus on melt/pivot/pivot_table first.\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(salary_pivot)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Tools for tidying data in Pandas: `stack()` / `unstack()`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "These functions are particularly useful when dealing with multi-dimensional data. While `melt()` and `pivot()` work with single-level column structures, `stack()` and `unstack()` are designed to work with hierarchical indexes and columns _(MultiIndex)_."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### `stack()`\n",
    "\n",
    "- Rotates _(or pivots)_ the innermost column level to become the innermost row index\n",
    "- Moves data from columns to row index\n",
    "- Can specify which level to stack using the level parameter\n",
    "- Returns a `Series` if one column level remains, `DataFrame` otherwise\n",
    "\n",
    "#### `unstack()`\n",
    "\n",
    "- Inverse operation of `stack()`\n",
    "- Rotates _(or pivots)_ the innermost row index level to become the rightmost column level\n",
    "- Moves data from row index to columns\n",
    "- Can specify which level to unstack using the level parameter"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Main differences from `melt()`/`pivot()`\n",
    "\n",
    "1. Data structure requirements\n",
    "   - `melt()`/`pivot()`: Work with single-level columns\n",
    "   - `stack()`/`unstack()`: Work with hierarchical indexes (MultiIndex)\n",
    "\n",
    "2. Operation style\n",
    "   - `melt()`/`pivot()`: Reshape between \"wide\" and \"long\" formats\n",
    "   - `stack()`/`unstack()`: Rotate levels between row index and columns\n",
    "\n",
    "3. Use cases\n",
    "   - `melt()`/`pivot()`: Better for single-level reshaping and general data restructuring\n",
    "   - `stack()`/`unstack()`: Better for working with hierarchical data and multi-dimensional analysis\n",
    "\n",
    "#### Common use cases\n",
    "\n",
    "1. Financial analysis\n",
    "   - Working with time series data with multiple categories\n",
    "   - Analyzing metrics across different dimensions _(time, region, product)_\n",
    "   - Creating financial reports with hierarchical structure\n",
    "\n",
    "2. Multi-dimensional data\n",
    "   - Survey responses with multiple categories\n",
    "   - Sales data across regions, products, and time periods\n",
    "   - Performance metrics with nested categories\n",
    "\n",
    "3. Data cleaning\n",
    "   - Reorganizing complex datasets\n",
    "   - Preparing data for specific analysis requirements\n",
    "   - Converting between different hierarchical structures\n",
    "\n",
    "#### Best practices\n",
    "\n",
    "1. Level management\n",
    "   - Be explicit about which levels you're stacking/unstacking\n",
    "   - Use level names or positions consistently\n",
    "   - Keep track of your index hierarchy\n",
    "\n",
    "2. Data integrity\n",
    "   - Check for missing values before and after operations\n",
    "   - Verify that the resulting structure matches your expectations\n",
    "   - Consider how aggregation should handle duplicates\n",
    "\n",
    "3. Performance\n",
    "   - Stack/unstack operations can be memory-intensive\n",
    "   - Consider working with subsets of large datasets\n",
    "   - Use appropriate data types to minimize memory usage"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Advanced DataFrame operations\n",
    "\n",
    "Following from the above _(i.e., reshaping and tidying data)_, let's have a look at more advanced `DataFrame` operations that are commonly used in data analysis. These operations enable complex calculations, create derived metrics, and analysze data across multiple dimensions.\n",
    "\n",
    "Advanced operations typically combine multiple basic Pandas functions to achieve more sophisticated analyses, including\n",
    "\n",
    "- Window functions and rolling calculations for trend analysis\n",
    "- Advanced data cleaning and feature engineering for derived insights\n",
    "- Multi-level indexing for hierarchical data organisation\n",
    "- Time series analysis for temporal patterns\n",
    "- Complex aggregations for summary statistics\n",
    "\n",
    "These techniques are particularly useful when working with real-world datasets that require more nuanced analysis than simple grouping or filtering operations."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Window functions and rolling calculations\n",
    "\n",
    "Window functions allow you to perform calculations across a set of rows that are related to the current row. Rolling calculations are a type of window function that operate over a sliding window of data.\n",
    "\n",
    "#### Key concepts\n",
    "\n",
    "- Group-based window calculations\n",
    "- Rolling statistics\n",
    "- Transformation of grouped data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Example\n",
    "\n",
    "We calculate a rolling mean of salaries within each department to show salary progression. This helps identify trends in compensation within organizational units."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# Calculate rolling mean of salary_2022 by department\ndf = df.sort_values(\"salary_2022\")\ndf[\"salary_rolling_mean\"] = df.groupby(\"department\")[\"salary_2022\"].transform(\n    lambda x: x.rolling(window=2, min_periods=1).mean()\n)"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"DataFrame with rolling mean salary by department:\")\n",
    "print(df[[\"name\", \"department\", \"salary_2022\", \"salary_rolling_mean\"]].head(6))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Advanced data cleaning\n",
    "\n",
    "This section demonstrates some advanced data cleaning techniques, including string manipulation and binning continuous data into categories."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Example\n",
    "\n",
    "We create derived metrics that provide additional insights:\n",
    "\n",
    "- Combined performance score from quarterly ratings\n",
    "- Performance categories using binning\n",
    "- Salary increase percentages\n",
    "- These transformations help in creating meaningful aggregations and visualisations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create combined performance score and categorize it\n",
    "df[\"avg_performance\"] = df[[\"performance_Q1\", \"performance_Q2\"]].mean(axis=1)\n",
    "df[\"performance_category\"] = pd.cut(\n",
    "    df[\"avg_performance\"],\n",
    "    bins=[0, 3.5, 4.0, 4.5, 5.0],\n",
    "    labels=[\n",
    "        \"Needs Improvement\",\n",
    "        \"Meets Expectations\",\n",
    "        \"Exceeds Expectations\",\n",
    "        \"Outstanding\"]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df[\"avg_performance\"].head(6)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate salary increase percentage\n",
    "df[\"salary_increase_pct\"] = (\n",
    "    (df[\"salary_2023\"] - df[\"salary_2022\"]) / df[\"salary_2022\"] * 100\n",
    ").round(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df[\"salary_increase_pct\"].head(6)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Advanced data cleaning results:\")\n",
    "print(df[[\"name\", \"avg_performance\", \"performance_category\", \"salary_increase_pct\"]].head(6))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Working with MultiIndex\n",
    "\n",
    "This section shows more advanced operations with MultiIndex `DataFrames`, including sorting and cross-sectional selection.\n",
    "\n",
    "The multi-level indexing allows us to:\n",
    "\n",
    "- Organise data hierarchically _(`department` \u2192 `city` \u2192 `employee`)_\n",
    "- Perform cross-sectional analysis\n",
    "- Access data at different levels of granularity"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Example"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a meaningful multi-level index\n",
    "df_multi = df.set_index([\"department\", \"city\", \"name\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Multi-level indexed DataFrame:\")\n",
    "print(df_multi.head(6))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Demonstrate cross-sectional selection\n",
    "print(\"Cross-section for IT department:\")\n",
    "print(df_multi.xs(\"IT\", level=\"department\")[[\"salary_2022\", \"salary_2023\", \"avg_performance\"]])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Time series resampling\n",
    "\n",
    "Resampling allows you to change the frequency of time series data. This is useful for aggregating high-frequency data to a lower frequency, or for converting between different time frequencies."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Example\n",
    "\n",
    "We reshape and analyze the quarterly performance data to:\n",
    "\n",
    "- Track changes over time\n",
    "- Compare departments\n",
    "- Identify trends in employee performance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a time-based analysis of performance scores\n",
    "performance_data = df.melt(\n",
    "    id_vars=[\"name\", \"department\"],\n",
    "    value_vars=[\"performance_Q1\", \"performance_Q2\"],\n",
    "    var_name=\"quarter\",\n",
    "    value_name=\"performance\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Clean up quarter names\n",
    "performance_data[\"quarter\"] = performance_data[\"quarter\"].str.replace(\"performance_\", \"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate department averages over time\n",
    "dept_performance = performance_data.groupby(\n",
    "    [\"department\", \"quarter\"])[\"performance\"].mean().round(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Department performance over time:\")\n",
    "print(dept_performance)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Advanced aggregation\n",
    "\n",
    "Aggregations are operations that summarise multiple rows of data into a single result. You can think of it as calculating metrics like averages, sums, or counts, but with the ability to perform multiple calculations simultaneously across different groups of data."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Example\n",
    "\n",
    "This example shows how to:\n",
    "\n",
    "- Perform multiple aggregations simultaneously\n",
    "- Create summary statistics by department\n",
    "- Combine different metrics into a comprehensive view"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Complex aggregation by department\n",
    "dept_summary = df.groupby(\"department\").agg({\n",
    "    \"salary_2022\": [\"mean\", \"min\", \"max\"],\n",
    "    \"avg_performance\": \"mean\",\n",
    "    \"training_hours\": \"sum\",\n",
    "    \"projects_completed\": \"sum\"\n",
    "}).round(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Department summary with multiple aggregations:\")\n",
    "print(dept_summary)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Advanced visualisation\n",
    "\n",
    "The last section demonstrates how to create more complex visualisations using Pandas and [Matplotlib](https://matplotlib.org/). Here, we're creating a simple scatter plot."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.plot(x=\"avg_performance\", y=\"salary_2023\", kind=\"scatter\")\n",
    "plt.title(\"Performance vs Salary\")\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}