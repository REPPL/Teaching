{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Advanced Pandas (Solutions)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "_This notebook provides solutions to the Week 05 exercises. Each solution includes explanations and, where appropriate, alternative approaches._\n",
    "\n",
    "Note: This Jupyter Notebook was originally compiled by Alex Reppel (AR) based on conversations with [ClaudeAI](https://claude.ai/) *(version 3.5 Sonnet)*. For this year's materials, further revisions were made using [Claude Code](https://www.anthropic.com/claude-code) *(Sonnet 4.5)*, including updated documentation and git commit messages."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Set display options for better output\n",
    "pd.set_option(\"display.max_columns\", None)\n",
    "pd.set_option(\"display.width\", None)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 1: Data reshaping (20 minutes)\n",
    "\n",
    "In this section, you'll practice converting data between wide and long formats using `melt()` and `pivot()`."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise 1: Sales data reshaping\n",
    "\n",
    "You have quarterly sales data in wide format. Convert it to long format for analysis.\n",
    "\n",
    "1. Use `melt()` to convert the quarterly columns to long format\n",
    "2. Clean the `quarter` column to remove the `sales_` prefix\n",
    "3. Display the first 10 rows of the melted data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sales data in wide format\n",
    "sales_wide = pd.DataFrame({\n",
    "    \"product\": [\"Laptop\", \"Mouse\", \"Keyboard\", \"Monitor\"],\n",
    "    \"region\": [\"North\", \"North\", \"South\", \"South\"],\n",
    "    \"sales_Q1\": [15000, 3000, 12000, 18000],\n",
    "    \"sales_Q2\": [18000, 3500, 14000, 20000],\n",
    "    \"sales_Q3\": [16000, 3200, 13000, 19000],\n",
    "    \"sales_Q4\": [20000, 4000, 15000, 22000]\n",
    "})\n",
    "\n",
    "print(\"Wide format:\")\n",
    "print(sales_wide)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Solution\n",
    "# Step 1: Melt the quarterly columns\n",
    "sales_long = sales_wide.melt(\n",
    "    id_vars=[\"product\", \"region\"],\n",
    "    value_vars=[\"sales_Q1\", \"sales_Q2\", \"sales_Q3\", \"sales_Q4\"],\n",
    "    var_name=\"quarter\",\n",
    "    value_name=\"sales\"\n",
    ")\n",
    "\n",
    "# Step 2: Clean the quarter column\n",
    "sales_long[\"quarter\"] = sales_long[\"quarter\"].str.replace(\"sales_\", \"\")\n",
    "\n",
    "# Step 3: Display first 10 rows\n",
    "print(\"Long format:\")\n",
    "print(sales_long.head(10))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Explanation**: The `melt()` function converts wide data to long format by:\n",
    "- Keeping `id_vars` columns as-is (product, region)\n",
    "- Taking the column names from `value_vars` and putting them in the `quarter` column\n",
    "- Taking the values from those columns and putting them in the `sales` column\n",
    "\n",
    "We then use `.str.replace()` to clean up the quarter names by removing the \"sales_\" prefix."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise 2: Employee performance data\n",
    "\n",
    "Convert long-format employee performance data back to wide format.\n",
    "\n",
    "1. Use `pivot()` to create a wide format with employees as rows and quarters as columns\n",
    "2. Reset the index to make `employee` a regular column\n",
    "3. Calculate the average performance for each employee across all quarters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Performance data in long format\n",
    "perf_long = pd.DataFrame({\n",
    "    \"employee\": [\"Alice\", \"Alice\", \"Alice\", \"Alice\", \n",
    "                 \"Bob\", \"Bob\", \"Bob\", \"Bob\",\n",
    "                 \"Carol\", \"Carol\", \"Carol\", \"Carol\"],\n",
    "    \"quarter\": [\"Q1\", \"Q2\", \"Q3\", \"Q4\"] * 3,\n",
    "    \"score\": [4.2, 4.5, 4.3, 4.6,\n",
    "              3.8, 4.0, 4.1, 3.9,\n",
    "              4.7, 4.8, 4.6, 4.9]\n",
    "})\n",
    "\n",
    "print(\"Long format:\")\n",
    "print(perf_long)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Solution\n",
    "# Step 1: Pivot to wide format\n",
    "perf_wide = perf_long.pivot(\n",
    "    index=\"employee\",\n",
    "    columns=\"quarter\",\n",
    "    values=\"score\"\n",
    ")\n",
    "\n",
    "# Step 2: Reset index\n",
    "perf_wide = perf_wide.reset_index()\n",
    "\n",
    "# Step 3: Calculate average performance\n",
    "perf_wide[\"average\"] = perf_wide[[\"Q1\", \"Q2\", \"Q3\", \"Q4\"]].mean(axis=1)\n",
    "\n",
    "print(\"Wide format with averages:\")\n",
    "print(perf_wide)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Explanation**: The `pivot()` function converts long data to wide format by:\n",
    "- Using `employee` as the row index\n",
    "- Using `quarter` values as column names\n",
    "- Filling the table with `score` values\n",
    "\n",
    "After pivoting, `employee` becomes the index, so we use `.reset_index()` to convert it back to a regular column.\n",
    "\n",
    "To calculate the average, we use `.mean(axis=1)` which calculates the mean across columns (axis=1) for each row."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise 3: Revenue by product and region\n",
    "\n",
    "Practice the melt-pivot workflow:\n",
    "\n",
    "1. Melt the revenue data to long format\n",
    "2. Pivot it back to show products as rows and months as columns\n",
    "3. Add a column showing total revenue for each product"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Revenue data\n",
    "revenue_data = pd.DataFrame({\n",
    "    \"product\": [\"Widget A\", \"Widget B\", \"Widget C\"],\n",
    "    \"region\": [\"East\", \"East\", \"West\"],\n",
    "    \"Jan\": [5000, 7000, 6000],\n",
    "    \"Feb\": [5500, 7500, 6500],\n",
    "    \"Mar\": [6000, 8000, 7000]\n",
    "})\n",
    "\n",
    "print(\"Original data:\")\n",
    "print(revenue_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Solution\n",
    "# Step 1: Melt to long format\n",
    "revenue_long = revenue_data.melt(\n",
    "    id_vars=[\"product\", \"region\"],\n",
    "    value_vars=[\"Jan\", \"Feb\", \"Mar\"],\n",
    "    var_name=\"month\",\n",
    "    value_name=\"revenue\"\n",
    ")\n",
    "\n",
    "print(\"Long format:\")\n",
    "print(revenue_long.head())\n",
    "\n",
    "# Step 2: Pivot back to wide format with products as rows\n",
    "revenue_wide = revenue_long.pivot(\n",
    "    index=\"product\",\n",
    "    columns=\"month\",\n",
    "    values=\"revenue\"\n",
    ").reset_index()\n",
    "\n",
    "# Step 3: Add total column\n",
    "revenue_wide[\"Total\"] = revenue_wide[[\"Jan\", \"Feb\", \"Mar\"]].sum(axis=1)\n",
    "\n",
    "print(\"\\nWide format with totals:\")\n",
    "print(revenue_wide)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Explanation**: This exercise demonstrates the common workflow of:\n",
    "1. Melt → long format (good for analysis)\n",
    "2. Pivot → wide format (good for reporting)\n",
    "\n",
    "Note that when we pivot, we lose the `region` information because pivot creates a single value for each product-month combination. If we needed to preserve region, we would include it in the index: `index=[\"product\", \"region\"]`.\n",
    "\n",
    "The `.sum(axis=1)` calculates row-wise totals across the month columns."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 2: Pivot tables and aggregation (20 minutes)\n",
    "\n",
    "Practice creating pivot tables and performing advanced aggregations."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise 4: Sales analysis by region and product\n",
    "\n",
    "Create a pivot table to analyse sales performance:\n",
    "\n",
    "1. Create a pivot table with `region` as rows and `product` as columns\n",
    "2. Show total sales for each region-product combination\n",
    "3. Add margins to show row and column totals"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sales transaction data\n",
    "sales_data = pd.DataFrame({\n",
    "    \"date\": pd.date_range(\"2024-01-01\", periods=20),\n",
    "    \"region\": [\"North\", \"South\", \"East\", \"West\"] * 5,\n",
    "    \"product\": [\"Laptop\", \"Mouse\", \"Keyboard\", \"Monitor\", \"Laptop\"] * 4,\n",
    "    \"sales_amount\": np.random.randint(1000, 10000, 20)\n",
    "})\n",
    "\n",
    "print(\"Sales data:\")\n",
    "print(sales_data.head(10))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Solution\n",
    "sales_pivot = pd.pivot_table(\n",
    "    sales_data,\n",
    "    values=\"sales_amount\",\n",
    "    index=\"region\",\n",
    "    columns=\"product\",\n",
    "    aggfunc=\"sum\",\n",
    "    margins=True,\n",
    "    margins_name=\"Total\"\n",
    ")\n",
    "\n",
    "print(\"Sales by region and product:\")\n",
    "print(sales_pivot)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Explanation**: `pivot_table()` is more powerful than `pivot()` because:\n",
    "- It can handle duplicate index-column combinations by aggregating them\n",
    "- It supports aggregation functions like `sum`, `mean`, `count`, etc.\n",
    "- It can add margins (totals) automatically\n",
    "\n",
    "The `margins=True` parameter adds:\n",
    "- A \"Total\" row showing column sums\n",
    "- A \"Total\" column showing row sums\n",
    "- A grand total in the bottom-right corner"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise 5: Multi-metric department analysis\n",
    "\n",
    "Create a comprehensive departmental summary:\n",
    "\n",
    "1. Use `pivot_table()` to analyse salary, performance, and project counts by department\n",
    "2. Apply different aggregation functions to each metric:\n",
    "   - Salary: mean\n",
    "   - Performance: mean, min, max\n",
    "   - Projects: sum\n",
    "3. Round the results to 2 decimal places"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Employee data\n",
    "employee_data = pd.DataFrame({\n",
    "    \"name\": [\"Alice\", \"Bob\", \"Carol\", \"Dan\", \"Eve\", \"Frank\", \"Grace\", \"Henry\"],\n",
    "    \"department\": [\"Sales\", \"IT\", \"HR\", \"Sales\", \"IT\", \"HR\", \"Sales\", \"IT\"],\n",
    "    \"salary\": [50000, 65000, 55000, 52000, 68000, 57000, 54000, 70000],\n",
    "    \"performance\": [4.2, 3.8, 4.5, 4.0, 4.1, 4.3, 3.9, 4.4],\n",
    "    \"projects_completed\": [5, 8, 6, 7, 9, 5, 6, 10]\n",
    "})\n",
    "\n",
    "print(\"Employee data:\")\n",
    "print(employee_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Solution\n",
    "dept_summary = pd.pivot_table(\n",
    "    employee_data,\n",
    "    values=[\"salary\", \"performance\", \"projects_completed\"],\n",
    "    index=\"department\",\n",
    "    aggfunc={\n",
    "        \"salary\": \"mean\",\n",
    "        \"performance\": [\"mean\", \"min\", \"max\"],\n",
    "        \"projects_completed\": \"sum\"\n",
    "    }\n",
    ").round(2)\n",
    "\n",
    "print(\"Department summary:\")\n",
    "print(dept_summary)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Explanation**: This demonstrates the power of `pivot_table()` for multi-metric analysis. The `aggfunc` parameter accepts:\n",
    "- A single function (e.g., `\"mean\"`)\n",
    "- A list of functions for all columns (e.g., `[\"mean\", \"sum\"]`)\n",
    "- A dictionary mapping specific columns to specific functions\n",
    "\n",
    "Using a dictionary allows us to apply different aggregations to different columns:\n",
    "- Salary: average makes sense (typical salary)\n",
    "- Performance: mean, min, max show the range within each department\n",
    "- Projects: sum shows total department output\n",
    "\n",
    "The `.round(2)` method rounds all numeric values to 2 decimal places for better readability."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise 6: Time-based sales analysis\n",
    "\n",
    "Analyse sales trends over time:\n",
    "\n",
    "1. Create a pivot table showing average sales by product and month\n",
    "2. Identify which product has the highest average monthly sales\n",
    "3. Calculate the month-over-month growth rate for each product"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Monthly sales data\n",
    "monthly_sales = pd.DataFrame({\n",
    "    \"date\": pd.date_range(\"2024-01-01\", periods=60, freq=\"D\"),\n",
    "    \"product\": [\"Laptop\", \"Mouse\", \"Keyboard\"] * 20,\n",
    "    \"sales\": np.random.randint(5000, 20000, 60)\n",
    "})\n",
    "\n",
    "# Extract month from date\n",
    "monthly_sales[\"month\"] = monthly_sales[\"date\"].dt.to_period(\"M\")\n",
    "\n",
    "print(\"Daily sales data:\")\n",
    "print(monthly_sales.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Solution\n",
    "# Step 1: Create pivot table of average sales\n",
    "sales_by_month = pd.pivot_table(\n",
    "    monthly_sales,\n",
    "    values=\"sales\",\n",
    "    index=\"product\",\n",
    "    columns=\"month\",\n",
    "    aggfunc=\"mean\"\n",
    ").round(0)\n",
    "\n",
    "print(\"Average sales by product and month:\")\n",
    "print(sales_by_month)\n",
    "\n",
    "# Step 2: Identify highest average monthly sales\n",
    "overall_avg = sales_by_month.mean(axis=1)\n",
    "print(f\"\\nOverall average sales by product:\")\n",
    "print(overall_avg)\n",
    "print(f\"\\nHighest average: {overall_avg.idxmax()} (£{overall_avg.max():.0f})\")\n",
    "\n",
    "# Step 3: Calculate month-over-month growth rate\n",
    "growth_rate = sales_by_month.pct_change(axis=1) * 100\n",
    "print(f\"\\nMonth-over-month growth rate (%):\")\n",
    "print(growth_rate.round(1))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Explanation**: This exercise shows time-based analysis patterns:\n",
    "\n",
    "1. **Creating the pivot table**: We use `aggfunc=\"mean\"` to average daily sales within each month\n",
    "\n",
    "2. **Finding the highest average**: \n",
    "   - `.mean(axis=1)` calculates the average across all months for each product\n",
    "   - `.idxmax()` returns the index label (product name) of the maximum value\n",
    "\n",
    "3. **Calculating growth rates**:\n",
    "   - `.pct_change(axis=1)` calculates percentage change across columns (months)\n",
    "   - `axis=1` is crucial—it compares each month to the previous month (left-to-right)\n",
    "   - Multiply by 100 to convert decimals to percentages\n",
    "\n",
    "The first month will have NaN for growth rate because there's no previous month to compare to."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 3: MultiIndex operations (15 minutes)\n",
    "\n",
    "Work with hierarchical indices to organise and analyse multi-dimensional data."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise 7: Creating and navigating MultiIndex\n",
    "\n",
    "Practice working with hierarchical indices:\n",
    "\n",
    "1. Create a MultiIndex DataFrame with `region` and `city` as index levels\n",
    "2. Sort the DataFrame by the index\n",
    "3. Select all data for the 'North' region using cross-section (`.xs()`)\n",
    "4. Calculate the total sales for each region"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Regional sales data\n",
    "regional_data = pd.DataFrame({\n",
    "    \"region\": [\"North\", \"North\", \"South\", \"South\", \"East\", \"East\"],\n",
    "    \"city\": [\"London\", \"Manchester\", \"Brighton\", \"Southampton\", \"Norwich\", \"Cambridge\"],\n",
    "    \"sales_Q1\": [15000, 12000, 18000, 14000, 16000, 13000],\n",
    "    \"sales_Q2\": [16000, 13000, 19000, 15000, 17000, 14000]\n",
    "})\n",
    "\n",
    "print(\"Regional sales data:\")\n",
    "print(regional_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Solution\n",
    "# Step 1: Create MultiIndex\n",
    "regional_multi = regional_data.set_index([\"region\", \"city\"])\n",
    "\n",
    "# Step 2: Sort by index\n",
    "regional_multi = regional_multi.sort_index()\n",
    "\n",
    "print(\"MultiIndex DataFrame:\")\n",
    "print(regional_multi)\n",
    "\n",
    "# Step 3: Select all data for North region\n",
    "print(\"\\nNorth region data:\")\n",
    "north_data = regional_multi.xs(\"North\", level=\"region\")\n",
    "print(north_data)\n",
    "\n",
    "# Step 4: Calculate total sales for each region\n",
    "print(\"\\nTotal sales by region:\")\n",
    "region_totals = regional_multi.groupby(level=\"region\").sum()\n",
    "print(region_totals)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Explanation**: MultiIndex provides hierarchical organization:\n",
    "\n",
    "1. **Creating MultiIndex**: `.set_index([\"region\", \"city\"])` creates two index levels\n",
    "\n",
    "2. **Sorting**: `.sort_index()` sorts by the index hierarchy (region first, then city within each region)\n",
    "\n",
    "3. **Cross-section selection**: `.xs()` selects data at a specific level:\n",
    "   - `xs(\"North\", level=\"region\")` selects all rows where region is \"North\"\n",
    "   - This returns a DataFrame with only the remaining index level (city)\n",
    "\n",
    "4. **Grouping by level**: When you have a MultiIndex, you can group by specific index levels:\n",
    "   - `groupby(level=\"region\")` groups by the region index level\n",
    "   - This sums all cities within each region"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise 8: Stack and unstack operations\n",
    "\n",
    "Practice reshaping with `stack()` and `unstack()`:\n",
    "\n",
    "1. Set `product` and `quarter` as a MultiIndex\n",
    "2. Use `unstack()` to move `quarter` to columns\n",
    "3. Calculate the total sales for each product\n",
    "4. Use `stack()` to convert back to long format"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Quarterly product sales\n",
    "product_sales = pd.DataFrame({\n",
    "    \"product\": [\"Widget A\", \"Widget A\", \"Widget A\", \"Widget A\",\n",
    "                \"Widget B\", \"Widget B\", \"Widget B\", \"Widget B\"],\n",
    "    \"quarter\": [\"Q1\", \"Q2\", \"Q3\", \"Q4\"] * 2,\n",
    "    \"sales\": [10000, 12000, 11000, 13000,\n",
    "              8000, 9000, 8500, 9500]\n",
    "})\n",
    "\n",
    "print(\"Product sales data:\")\n",
    "print(product_sales)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Solution\n",
    "# Step 1: Create MultiIndex\n",
    "sales_multi = product_sales.set_index([\"product\", \"quarter\"])\n",
    "print(\"MultiIndex format:\")\n",
    "print(sales_multi)\n",
    "\n",
    "# Step 2: Unstack quarter to columns\n",
    "sales_unstacked = sales_multi.unstack()\n",
    "print(\"\\nUnstacked (wide format):\")\n",
    "print(sales_unstacked)\n",
    "\n",
    "# Step 3: Calculate total sales for each product\n",
    "sales_unstacked[\"Total\"] = sales_unstacked.sum(axis=1)\n",
    "print(\"\\nWith totals:\")\n",
    "print(sales_unstacked)\n",
    "\n",
    "# Step 4: Stack back to long format\n",
    "sales_stacked = sales_unstacked.stack()\n",
    "print(\"\\nStacked back to long format:\")\n",
    "print(sales_stacked)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Explanation**: `stack()` and `unstack()` are powerful reshaping operations:\n",
    "\n",
    "1. **Unstack**: Moves the innermost index level to columns\n",
    "   - Creates a wider DataFrame\n",
    "   - Useful for comparison and calculation across categories\n",
    "   - Note the hierarchical column names: (`sales`, `Q1`), (`sales`, `Q2`), etc.\n",
    "\n",
    "2. **Adding totals**: Once unstacked, we can easily calculate row-wise totals\n",
    "\n",
    "3. **Stack**: Moves column level back to index\n",
    "   - Creates a taller, narrower DataFrame\n",
    "   - The inverse of `unstack()`\n",
    "   - Returns a Series with MultiIndex\n",
    "\n",
    "**Note**: When you add a \"Total\" column before stacking, it becomes part of the stacked data. In practice, you might want to remove it first or use `.iloc[:, :-1]` to exclude it."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 4: Window functions and advanced operations (20 minutes)\n",
    "\n",
    "Apply window functions and advanced transformations to analyse trends."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise 9: Rolling averages for trend analysis\n",
    "\n",
    "Calculate rolling statistics to identify trends:\n",
    "\n",
    "1. Calculate a 7-day rolling average of daily sales\n",
    "2. Calculate a 7-day rolling standard deviation\n",
    "3. Identify days where sales are more than 2 standard deviations above the rolling mean"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Daily sales data\n",
    "np.random.seed(42)\n",
    "daily_sales = pd.DataFrame({\n",
    "    \"date\": pd.date_range(\"2024-01-01\", periods=30),\n",
    "    \"sales\": np.random.randint(5000, 15000, 30)\n",
    "})\n",
    "\n",
    "print(\"Daily sales:\")\n",
    "print(daily_sales.head(10))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Solution\n",
    "# Step 1: Calculate 7-day rolling average\n",
    "daily_sales[\"rolling_mean\"] = daily_sales[\"sales\"].rolling(window=7).mean()\n",
    "\n",
    "# Step 2: Calculate 7-day rolling standard deviation\n",
    "daily_sales[\"rolling_std\"] = daily_sales[\"sales\"].rolling(window=7).std()\n",
    "\n",
    "# Step 3: Identify outliers (> 2 std above mean)\n",
    "daily_sales[\"upper_bound\"] = daily_sales[\"rolling_mean\"] + 2 * daily_sales[\"rolling_std\"]\n",
    "daily_sales[\"is_outlier\"] = daily_sales[\"sales\"] > daily_sales[\"upper_bound\"]\n",
    "\n",
    "print(\"Sales with rolling statistics:\")\n",
    "print(daily_sales)\n",
    "\n",
    "print(\"\\nOutlier days:\")\n",
    "outliers = daily_sales[daily_sales[\"is_outlier\"] == True]\n",
    "print(outliers[[\"date\", \"sales\", \"rolling_mean\", \"upper_bound\"]])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Explanation**: Rolling window calculations are essential for trend analysis:\n",
    "\n",
    "1. **Rolling mean**: `.rolling(window=7).mean()` calculates the average of the current row and the previous 6 rows (7 days total)\n",
    "   - First 6 rows will be NaN because there aren't enough previous values\n",
    "   - Smooths out daily fluctuations to show the trend\n",
    "\n",
    "2. **Rolling standard deviation**: `.rolling(window=7).std()` measures variability over the window\n",
    "   - Higher values indicate more volatile sales\n",
    "   - Lower values indicate stable sales\n",
    "\n",
    "3. **Outlier detection**: The \"2 standard deviations\" rule:\n",
    "   - In a normal distribution, ~95% of values fall within 2 std of the mean\n",
    "   - Values beyond this are statistical outliers\n",
    "   - Useful for detecting unusual spikes or drops in sales\n",
    "\n",
    "**Business application**: This helps identify exceptional sales days that might warrant investigation (successful promotion, seasonal effect, data error, etc.)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise 10: Group-wise transformations\n",
    "\n",
    "Apply transformations within groups:\n",
    "\n",
    "1. Calculate each employee's salary as a percentage of their department's total salary\n",
    "2. Calculate each employee's ranking within their department based on performance\n",
    "3. Create a column showing the department average performance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Employee performance data\n",
    "employees = pd.DataFrame({\n",
    "    \"name\": [\"Alice\", \"Bob\", \"Carol\", \"Dan\", \"Eve\", \"Frank\"],\n",
    "    \"department\": [\"Sales\", \"Sales\", \"IT\", \"IT\", \"HR\", \"HR\"],\n",
    "    \"salary\": [50000, 55000, 65000, 70000, 48000, 52000],\n",
    "    \"performance\": [4.2, 4.5, 3.8, 4.1, 4.6, 4.3]\n",
    "})\n",
    "\n",
    "print(\"Employee data:\")\n",
    "print(employees)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Solution\n",
    "# Step 1: Salary as percentage of department total\n",
    "employees[\"dept_total_salary\"] = employees.groupby(\"department\")[\"salary\"].transform(\"sum\")\n",
    "employees[\"salary_pct\"] = (employees[\"salary\"] / employees[\"dept_total_salary\"] * 100).round(1)\n",
    "\n",
    "# Step 2: Ranking within department by performance\n",
    "employees[\"dept_rank\"] = employees.groupby(\"department\")[\"performance\"].rank(ascending=False, method=\"min\")\n",
    "\n",
    "# Step 3: Department average performance\n",
    "employees[\"dept_avg_performance\"] = employees.groupby(\"department\")[\"performance\"].transform(\"mean\").round(2)\n",
    "\n",
    "print(\"Employee data with transformations:\")\n",
    "print(employees)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Explanation**: The `.transform()` method is powerful for group-wise operations:\n",
    "\n",
    "1. **Salary percentage**:\n",
    "   - `.transform(\"sum\")` calculates the sum for each group and broadcasts it back to all rows\n",
    "   - Each employee gets their department's total salary\n",
    "   - We then calculate the individual percentage: `(individual / total) * 100`\n",
    "\n",
    "2. **Department ranking**:\n",
    "   - `.rank(ascending=False)` ranks highest performance as 1\n",
    "   - `method=\"min\"` handles ties by giving them the minimum rank\n",
    "   - Ranking is done within each department group\n",
    "\n",
    "3. **Department average**:\n",
    "   - `.transform(\"mean\")` calculates the mean for each department\n",
    "   - Broadcasts it back so each employee has their department's average\n",
    "   - Useful for comparing individual vs. department performance\n",
    "\n",
    "**Why use `.transform()` vs `.agg()`?**\n",
    "- `.agg()` returns one row per group (reduces rows)\n",
    "- `.transform()` returns the same number of rows as the original DataFrame (broadcasts values)\n",
    "- Use `.transform()` when you need to keep the original row structure"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 5: Data cleaning and transformation (15 minutes)\n",
    "\n",
    "Practice advanced data cleaning and feature engineering techniques."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise 11: Binning and categorization\n",
    "\n",
    "Create meaningful categories from continuous data:\n",
    "\n",
    "1. Use `pd.cut()` to create age groups: 'Young' (< 30), 'Mid-career' (30-45), 'Senior' (> 45)\n",
    "2. Use `pd.qcut()` to create salary quartiles\n",
    "3. Create a pivot table showing the count of employees in each age group by salary quartile"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Employee demographics\n",
    "demographics = pd.DataFrame({\n",
    "    \"name\": [\"Alice\", \"Bob\", \"Carol\", \"Dan\", \"Eve\", \"Frank\", \"Grace\", \"Henry\",\n",
    "             \"Ivy\", \"Jack\", \"Kelly\", \"Liam\"],\n",
    "    \"age\": [25, 28, 35, 42, 31, 48, 26, 39, 44, 52, 29, 36],\n",
    "    \"salary\": [45000, 52000, 68000, 85000, 58000, 92000, 47000, 72000,\n",
    "               88000, 95000, 51000, 70000]\n",
    "})\n",
    "\n",
    "print(\"Employee demographics:\")\n",
    "print(demographics)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Solution\n",
    "# Step 1: Create age groups using pd.cut()\n",
    "demographics[\"age_group\"] = pd.cut(\n",
    "    demographics[\"age\"],\n",
    "    bins=[0, 30, 45, 100],\n",
    "    labels=[\"Young\", \"Mid-career\", \"Senior\"]\n",
    ")\n",
    "\n",
    "# Step 2: Create salary quartiles using pd.qcut()\n",
    "demographics[\"salary_quartile\"] = pd.qcut(\n",
    "    demographics[\"salary\"],\n",
    "    q=4,\n",
    "    labels=[\"Q1\", \"Q2\", \"Q3\", \"Q4\"]\n",
    ")\n",
    "\n",
    "print(\"With categories:\")\n",
    "print(demographics)\n",
    "\n",
    "# Step 3: Create pivot table\n",
    "age_salary_table = pd.pivot_table(\n",
    "    demographics,\n",
    "    values=\"name\",\n",
    "    index=\"age_group\",\n",
    "    columns=\"salary_quartile\",\n",
    "    aggfunc=\"count\",\n",
    "    fill_value=0\n",
    ")\n",
    "\n",
    "print(\"\\nEmployee count by age group and salary quartile:\")\n",
    "print(age_salary_table)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Explanation**: Binning converts continuous data into categorical groups:\n",
    "\n",
    "1. **`pd.cut()` - Fixed width bins**:\n",
    "   - You specify the bin edges: `[0, 30, 45, 100]`\n",
    "   - Creates categories: 0-30, 30-45, 45-100\n",
    "   - Use when you have natural cut points (e.g., age groups, score ranges)\n",
    "   - Good for business rules or policy-based groupings\n",
    "\n",
    "2. **`pd.qcut()` - Quantile-based bins**:\n",
    "   - You specify the number of bins: `q=4`\n",
    "   - Pandas automatically creates equal-sized groups (quartiles)\n",
    "   - Each quartile contains approximately the same number of observations\n",
    "   - Use for statistical analysis or when you want evenly distributed groups\n",
    "\n",
    "3. **The pivot table**:\n",
    "   - Shows the distribution of employees across age groups and salary levels\n",
    "   - `fill_value=0` replaces NaN with 0 for empty combinations\n",
    "   - Helps identify patterns (e.g., are senior employees in higher quartiles?)\n",
    "\n",
    "**Business application**: This type of analysis helps with:\n",
    "- Compensation planning\n",
    "- Identifying pay equity issues\n",
    "- Understanding career progression patterns"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise 12: Method chaining for data pipelines\n",
    "\n",
    "Create an efficient data processing pipeline using method chaining:\n",
    "\n",
    "1. Filter to include only sales above 5000\n",
    "2. Create a new column `revenue_category` based on sales:\n",
    "   - 'Low': < 7500\n",
    "   - 'Medium': 7500-12500\n",
    "   - 'High': > 12500\n",
    "3. Group by `product` and `revenue_category` and count transactions\n",
    "4. Sort by count in descending order\n",
    "\n",
    "Try to complete this in a single chained expression."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Transaction data\n",
    "transactions = pd.DataFrame({\n",
    "    \"product\": [\"Laptop\", \"Mouse\", \"Keyboard\", \"Monitor\", \"Laptop\",\n",
    "                \"Mouse\", \"Keyboard\", \"Monitor\", \"Laptop\", \"Mouse\"] * 3,\n",
    "    \"sales\": [15000, 3000, 8000, 18000, 12000,\n",
    "              4000, 6000, 20000, 16000, 3500] * 3\n",
    "})\n",
    "\n",
    "print(\"Transaction data:\")\n",
    "print(transactions.head(10))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Solution (method chaining)\n",
    "result = (\n",
    "    transactions\n",
    "    .query(\"sales > 5000\")  # Step 1: Filter\n",
    "    .assign(  # Step 2: Create revenue_category\n",
    "        revenue_category=lambda x: pd.cut(\n",
    "            x[\"sales\"],\n",
    "            bins=[0, 7500, 12500, 100000],\n",
    "            labels=[\"Low\", \"Medium\", \"High\"]\n",
    "        )\n",
    "    )\n",
    "    .groupby([\"product\", \"revenue_category\"])  # Step 3: Group and count\n",
    "    .size()\n",
    "    .reset_index(name=\"count\")\n",
    "    .sort_values(\"count\", ascending=False)  # Step 4: Sort\n",
    ")\n",
    "\n",
    "print(\"Product-category analysis:\")\n",
    "print(result)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Explanation**: Method chaining creates readable, efficient pipelines:\n",
    "\n",
    "1. **`.query()`**: Filters using a string expression\n",
    "   - More readable than boolean indexing for simple conditions\n",
    "   - Works well in chains\n",
    "\n",
    "2. **`.assign()`**: Creates new columns\n",
    "   - Can use lambda functions to reference the DataFrame\n",
    "   - `lambda x: pd.cut(x[\"sales\"], ...)` creates categories based on sales column\n",
    "\n",
    "3. **`.groupby().size()`**: Counts rows in each group\n",
    "   - `.size()` counts all rows (including NaN)\n",
    "   - `.reset_index(name=\"count\")` converts the result to a DataFrame with a \"count\" column\n",
    "\n",
    "4. **`.sort_values()`**: Sorts by count descending\n",
    "\n",
    "**Benefits of method chaining**:\n",
    "- Reads top-to-bottom like a recipe\n",
    "- No intermediate variables cluttering the namespace\n",
    "- Easy to add/remove steps\n",
    "- Parentheses allow multi-line formatting for readability\n",
    "\n",
    "**Note**: The parentheses around the entire expression allow Python to treat multiple lines as one statement."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Alternative solution (step-by-step)\n",
    "# This achieves the same result but is less concise\n",
    "\n",
    "# Step 1: Filter\n",
    "filtered = transactions[transactions[\"sales\"] > 5000]\n",
    "\n",
    "# Step 2: Create category\n",
    "filtered[\"revenue_category\"] = pd.cut(\n",
    "    filtered[\"sales\"],\n",
    "    bins=[0, 7500, 12500, 100000],\n",
    "    labels=[\"Low\", \"Medium\", \"High\"]\n",
    ")\n",
    "\n",
    "# Step 3: Group and count\n",
    "grouped = filtered.groupby([\"product\", \"revenue_category\"]).size().reset_index(name=\"count\")\n",
    "\n",
    "# Step 4: Sort\n",
    "result_alt = grouped.sort_values(\"count\", ascending=False)\n",
    "\n",
    "print(\"Alternative solution (same result):\")\n",
    "print(result_alt)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Key takeaways\n",
    "\n",
    "### Data reshaping\n",
    "\n",
    "- **`melt()`** converts wide → long format (analysis-friendly)\n",
    "- **`pivot()`** converts long → wide format (reporting-friendly)\n",
    "- **Remember**: Wide format is good for humans, long format is good for analysis\n",
    "\n",
    "### Pivot tables and aggregation\n",
    "\n",
    "- **`pivot_table()`** is more powerful than `pivot()`—it aggregates duplicates\n",
    "- Use **dictionaries in `aggfunc`** to apply different functions to different columns\n",
    "- **`margins=True`** adds row and column totals automatically\n",
    "- **`.pct_change()`** calculates growth rates (use `axis=1` for row-wise changes)\n",
    "\n",
    "### MultiIndex operations\n",
    "\n",
    "- **MultiIndex** organises hierarchical data (Country → Region → City)\n",
    "- **`.xs()`** selects data at a specific index level (cross-section)\n",
    "- **`unstack()`** moves index → columns (wider)\n",
    "- **`stack()`** moves columns → index (taller)\n",
    "- **`groupby(level=)`** groups by specific index levels\n",
    "\n",
    "### Window functions and transformations\n",
    "\n",
    "- **`.rolling(window=N)`** calculates statistics over N rows\n",
    "- Common rolling operations: `.mean()`, `.std()`, `.sum()`\n",
    "- **`.transform()`** applies functions within groups, returns same-shaped data\n",
    "- **`.agg()`** reduces groups to summary statistics\n",
    "- Use **2 standard deviations** for outlier detection\n",
    "\n",
    "### Data cleaning and transformation\n",
    "\n",
    "- **`pd.cut()`** creates bins with specified edges (fixed-width)\n",
    "- **`pd.qcut()`** creates bins with equal counts (quantile-based)\n",
    "- **Method chaining** creates readable pipelines:\n",
    "  - `.query()` for filtering\n",
    "  - `.assign()` for creating columns\n",
    "  - `.groupby()` for aggregation\n",
    "  - `.sort_values()` for sorting\n",
    "\n",
    "### Common patterns\n",
    "\n",
    "| Task | Method | Example |\n",
    "|------|--------|--------|\n",
    "| Wide → Long | `melt()` | `df.melt(id_vars=[\"id\"], value_vars=[\"Q1\", \"Q2\"])` |\n",
    "| Long → Wide | `pivot()` | `df.pivot(index=\"id\", columns=\"quarter\", values=\"sales\")` |\n",
    "| Aggregate table | `pivot_table()` | `pd.pivot_table(df, values=\"sales\", index=\"region\", aggfunc=\"sum\")` |\n",
    "| Rolling average | `.rolling()` | `df[\"sales\"].rolling(window=7).mean()` |\n",
    "| Group transform | `.transform()` | `df.groupby(\"dept\")[\"salary\"].transform(\"mean\")` |\n",
    "| Create bins | `pd.cut()` | `pd.cut(df[\"age\"], bins=[0, 30, 50, 100])` |\n",
    "| Quantile bins | `pd.qcut()` | `pd.qcut(df[\"salary\"], q=4)` |\n",
    "\n",
    "### When to use each technique\n",
    "\n",
    "**Use `melt()` when**:\n",
    "- Creating visualizations\n",
    "- Performing statistical analysis\n",
    "- Preparing data for machine learning\n",
    "\n",
    "**Use `pivot()` or `pivot_table()` when**:\n",
    "- Creating summary tables for reports\n",
    "- Comparing values across categories\n",
    "- Calculating period-over-period changes\n",
    "\n",
    "**Use MultiIndex when**:\n",
    "- Data has natural hierarchies\n",
    "- Need multi-dimensional analysis\n",
    "- Working with time series at multiple frequencies\n",
    "\n",
    "**Use rolling functions when**:\n",
    "- Analysing trends over time\n",
    "- Smoothing noisy data\n",
    "- Detecting outliers\n",
    "\n",
    "**Use `.transform()` when**:\n",
    "- Need group statistics alongside individual rows\n",
    "- Calculating percentages of group totals\n",
    "- Comparing individuals to group averages"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
